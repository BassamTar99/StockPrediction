{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize an LSTM network from scratch\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            Size of the input features\n",
    "        hidden_size : int\n",
    "            Size of the hidden state\n",
    "        output_size : int\n",
    "            Size of the output\n",
    "        learning_rate : float\n",
    "            Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights for the forget gate\n",
    "        self.Wf = np.random.randn(hidden_size, hidden_size + input_size) * 0.01\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize weights for the input gate\n",
    "        self.Wi = np.random.randn(hidden_size, hidden_size + input_size) * 0.01\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize weights for the cell gate (candidate values)\n",
    "        self.Wc = np.random.randn(hidden_size, hidden_size + input_size) * 0.01\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize weights for the output gate\n",
    "        self.Wo = np.random.randn(hidden_size, hidden_size + input_size) * 0.01\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize weights for the output layer\n",
    "        self.Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -15, 15)))  # Clip to avoid overflow\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : list of numpy arrays\n",
    "            List of input vectors for each time step\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        h_states : list of numpy arrays\n",
    "            List of hidden states for each time step\n",
    "        outputs : list of numpy arrays\n",
    "            List of output vectors for each time step\n",
    "        \"\"\"\n",
    "        # Store inputs for backpropagation\n",
    "        self.inputs = inputs\n",
    "        T = len(inputs)  # Number of time steps\n",
    "\n",
    "        # Initialize lists to store states and gates\n",
    "        self.f_gates = []  # Forget gates\n",
    "        self.i_gates = []  # Input gates\n",
    "        self.c_tildes = []  # Candidate cell states\n",
    "        self.o_gates = []  # Output gates\n",
    "        self.c_states = []  # Cell states\n",
    "        self.h_states = []  # Hidden states\n",
    "        outputs = []  # Outputs\n",
    "\n",
    "        # Initialize the first hidden state and cell state with zeros\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "        c_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Forward pass for each time step\n",
    "        for t in range(T):\n",
    "            # Ensure the input is a column vector\n",
    "            x_t = np.array(inputs[t]).reshape(-1, 1)\n",
    "\n",
    "            # Concatenate previous hidden state with current input\n",
    "            concat = np.vstack((h_prev, x_t))\n",
    "\n",
    "            # Forget gate: determines what to forget from the cell state\n",
    "            # f_t = sigmoid(W_f · [h_{t-1}, x_t] + b_f)\n",
    "            f_t = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
    "            self.f_gates.append(f_t)\n",
    "\n",
    "            # Input gate: determines what new information to store\n",
    "            # i_t = sigmoid(W_i · [h_{t-1}, x_t] + b_i)\n",
    "            i_t = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
    "            self.i_gates.append(i_t)\n",
    "\n",
    "            # Candidate cell state: creates vector of new candidate values\n",
    "            # c̃_t = tanh(W_c · [h_{t-1}, x_t] + b_c)\n",
    "            c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n",
    "            self.c_tildes.append(c_tilde)\n",
    "\n",
    "            # Output gate: determines what to output from the cell state\n",
    "            # o_t = sigmoid(W_o · [h_{t-1}, x_t] + b_o)\n",
    "            o_t = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
    "            self.o_gates.append(o_t)\n",
    "\n",
    "            # New cell state: forget old information and add new information\n",
    "            # c_t = f_t * c_{t-1} + i_t * c̃_t\n",
    "            c_t = f_t * c_prev + i_t * c_tilde\n",
    "            self.c_states.append(c_t)\n",
    "\n",
    "            # New hidden state: filtered cell state through output gate\n",
    "            # h_t = o_t * tanh(c_t)\n",
    "            h_t = o_t * np.tanh(c_t)\n",
    "            self.h_states.append(h_t)\n",
    "\n",
    "            # Calculate output\n",
    "            # y_t = W_y · h_t + b_y\n",
    "            y_t = np.dot(self.Wy, h_t) + self.by\n",
    "            outputs.append(y_t)\n",
    "\n",
    "            # Update for next time step\n",
    "            h_prev = h_t\n",
    "            c_prev = c_t\n",
    "\n",
    "        return self.h_states, outputs\n",
    "\n",
    "    def backward(self, targets):\n",
    "        \"\"\"\n",
    "        Backpropagation through time (BPTT) for LSTM\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        targets : list of numpy arrays\n",
    "            List of target vectors for each time step\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "            Mean squared error loss\n",
    "        \"\"\"\n",
    "        T = len(targets)  # Number of time steps\n",
    "\n",
    "        # Initialize gradient accumulators\n",
    "        dWf, dWi, dWc, dWo, dWy = np.zeros_like(self.Wf), np.zeros_like(self.Wi), np.zeros_like(self.Wc), np.zeros_like(self.Wo), np.zeros_like(self.Wy)\n",
    "        dbf, dbi, dbc, dbo, dby = np.zeros_like(self.bf), np.zeros_like(self.bi), np.zeros_like(self.bc), np.zeros_like(self.bo), np.zeros_like(self.by)\n",
    "\n",
    "        # Initialize loss\n",
    "        loss = 0\n",
    "\n",
    "        # Initialize gradients for the next time step\n",
    "        dh_next = np.zeros_like(self.h_states[0])\n",
    "        dc_next = np.zeros_like(self.c_states[0])\n",
    "\n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(T)):\n",
    "            # Convert target to correct shape\n",
    "            target_t = np.array(targets[t]).reshape(-1, 1)\n",
    "\n",
    "            # Calculate output for current time step\n",
    "            y_t = np.dot(self.Wy, self.h_states[t]) + self.by\n",
    "\n",
    "            # Calculate error and loss\n",
    "            error = y_t - target_t\n",
    "            loss += np.sum(error ** 2) / 2\n",
    "\n",
    "            # Gradient of output weights\n",
    "            dWy += np.dot(error, self.h_states[t].T)\n",
    "            dby += error\n",
    "\n",
    "            # Gradient of hidden state\n",
    "            dh = np.dot(self.Wy.T, error) + dh_next\n",
    "\n",
    "            # Get the current gates and states\n",
    "            o_t = self.o_gates[t]\n",
    "            c_t = self.c_states[t]\n",
    "            c_tilde = self.c_tildes[t]\n",
    "            i_t = self.i_gates[t]\n",
    "            f_t = self.f_gates[t]\n",
    "\n",
    "            # Get previous cell state and hidden state\n",
    "            c_prev = np.zeros_like(self.c_states[0]) if t == 0 else self.c_states[t-1]\n",
    "            h_prev = np.zeros_like(self.h_states[0]) if t == 0 else self.h_states[t-1]\n",
    "\n",
    "            # Gradient of output gate\n",
    "            do = dh * np.tanh(c_t)\n",
    "            do_input = do * o_t * (1 - o_t)  # Derivative of sigmoid\n",
    "\n",
    "            # Gradient of cell state\n",
    "            dc = dc_next + dh * o_t * (1 - np.tanh(c_t)**2)\n",
    "\n",
    "            # Gradient of candidate cell state\n",
    "            dc_tilde = dc * i_t\n",
    "            dc_tilde_input = dc_tilde * (1 - c_tilde**2)  # Derivative of tanh\n",
    "\n",
    "            # Gradient of input gate\n",
    "            di = dc * c_tilde\n",
    "            di_input = di * i_t * (1 - i_t)  # Derivative of sigmoid\n",
    "\n",
    "            # Gradient of forget gate\n",
    "            df = dc * c_prev\n",
    "            df_input = df * f_t * (1 - f_t)  # Derivative of sigmoid\n",
    "\n",
    "            # Prepare next cell state gradient\n",
    "            dc_next = dc * f_t\n",
    "\n",
    "            # Concatenate previous hidden state with current input\n",
    "            x_t = np.array(self.inputs[t]).reshape(-1, 1)\n",
    "            concat = np.vstack((h_prev, x_t))\n",
    "\n",
    "            # Accumulate gradients for weights\n",
    "            dWf += np.dot(df_input, concat.T)\n",
    "            dWi += np.dot(di_input, concat.T)\n",
    "            dWc += np.dot(dc_tilde_input, concat.T)\n",
    "            dWo += np.dot(do_input, concat.T)\n",
    "\n",
    "            # Accumulate gradients for biases\n",
    "            dbf += df_input\n",
    "            dbi += di_input\n",
    "            dbc += dc_tilde_input\n",
    "            dbo += do_input\n",
    "\n",
    "            # Gradient for next hidden state\n",
    "            dconcat = np.dot(self.Wf.T, df_input) + np.dot(self.Wi.T, di_input) + \\\n",
    "                     np.dot(self.Wc.T, dc_tilde_input) + np.dot(self.Wo.T, do_input)\n",
    "\n",
    "            dh_next = dconcat[:self.hidden_size]\n",
    "\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        for dparam in [dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "        # Update parameters\n",
    "        self.Wf -= self.learning_rate * dWf\n",
    "        self.Wi -= self.learning_rate * dWi\n",
    "        self.Wc -= self.learning_rate * dWc\n",
    "        self.Wo -= self.learning_rate * dWo\n",
    "        self.Wy -= self.learning_rate * dWy\n",
    "\n",
    "        self.bf -= self.learning_rate * dbf\n",
    "        self.bi -= self.learning_rate * dbi\n",
    "        self.bc -= self.learning_rate * dbc\n",
    "        self.bo -= self.learning_rate * dbo\n",
    "        self.by -= self.learning_rate * dby\n",
    "\n",
    "        return loss / T\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=100):\n",
    "        \"\"\"\n",
    "        Train the LSTM on the given data\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : list of lists\n",
    "            List of input sequences\n",
    "        y_train : list of lists\n",
    "            List of target sequences\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        losses : list\n",
    "            List of losses for each epoch\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(len(X_train)):\n",
    "                # Forward pass\n",
    "                _, _ = self.forward(X_train[i])\n",
    "\n",
    "                # Backward pass\n",
    "                loss = self.backward(y_train[i])\n",
    "                total_loss += loss\n",
    "\n",
    "            avg_loss = total_loss / len(X_train)\n",
    "            losses.append(avg_loss)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        return losses\n",
    "\n",
    "# Demonstration with a specific problem: Learning a simple pattern\n",
    "def generate_pattern_data(samples, pattern_length=3):\n",
    "    \"\"\"Generate a repeating pattern with some noise\"\"\"\n",
    "    pattern = np.sin(np.linspace(0, 2 * np.pi, pattern_length))\n",
    "\n",
    "    # Repeat the pattern\n",
    "    repetitions = int(np.ceil(samples / pattern_length))\n",
    "    extended_pattern = np.tile(pattern, repetitions)[:samples]\n",
    "\n",
    "    # Add some noise\n",
    "    noise = np.random.normal(0, 0.05, samples)\n",
    "    noisy_pattern = extended_pattern + noise\n",
    "\n",
    "    return noisy_pattern\n",
    "\n",
    "def prepare_sequences(data, seq_length):\n",
    "    \"\"\"Prepare input-output sequences for time series prediction\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        # Input sequence\n",
    "        X.append(data[i:i+seq_length])\n",
    "        # Target sequence (next values)\n",
    "        y.append(data[i+1:i+seq_length+1])\n",
    "    return X, y\n",
    "\n",
    "def run_pattern_example():\n",
    "    print(\"LSTM from Scratch: Learning a Repeating Pattern\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Generate data with a repeating pattern\n",
    "    pattern_length = 3\n",
    "    total_samples = 200\n",
    "    data = generate_pattern_data(total_samples, pattern_length)\n",
    "\n",
    "    # Prepare sequences\n",
    "    seq_length = 10\n",
    "    X, y = prepare_sequences(data, seq_length)\n",
    "\n",
    "    # Split into train and test\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    # Display information\n",
    "    print(f\"Pattern length: {pattern_length}\")\n",
    "    print(f\"Sequence length: {seq_length}\")\n",
    "    print(f\"Training sequences: {len(X_train)}\")\n",
    "    print(f\"Testing sequences: {len(X_test)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Create and train LSTM\n",
    "    lstm = LSTM(input_size=1, hidden_size=10, output_size=1, learning_rate=0.01)\n",
    "    losses = lstm.train(X_train, y_train, epochs=200)\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title('LSTM Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Test with a sequence and predict future values\n",
    "    test_seq = X_test[0]\n",
    "    actual_future = y_test[0]\n",
    "\n",
    "    # Initial prediction\n",
    "    _, outputs = lstm.forward(test_seq)\n",
    "    predictions = [out[0][0] for out in outputs]\n",
    "\n",
    "    # Continue predicting beyond known data\n",
    "    extended_predictions = predictions.copy()\n",
    "    current_seq = test_seq.copy()\n",
    "\n",
    "    for _ in range(30):  # Predict 30 steps into the future\n",
    "        # Update sequence with the last prediction\n",
    "        current_seq = np.append(current_seq[1:], extended_predictions[-1])\n",
    "\n",
    "        # Get new prediction\n",
    "        _, new_outputs = lstm.forward(current_seq)\n",
    "        next_pred = new_outputs[-1][0][0]\n",
    "        extended_predictions.append(next_pred)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot original full pattern for reference\n",
    "    time_steps = range(total_samples)\n",
    "    plt.plot(time_steps, data, 'g-', alpha=0.3, label='Full Pattern')\n",
    "\n",
    "    # Plot the test sequence\n",
    "    start_idx = train_size + seq_length - 1\n",
    "    test_idxs = range(start_idx, start_idx + len(test_seq))\n",
    "    plt.plot(test_idxs, test_seq, 'b-', linewidth=2, label='Test Sequence')\n",
    "\n",
    "    # Plot actual future values\n",
    "    future_idxs = range(start_idx + 1, start_idx + 1 + len(actual_future))\n",
    "    plt.plot(future_idxs, actual_future, 'k-', linewidth=2, label='Actual Future')\n",
    "\n",
    "    # Plot one-step predictions\n",
    "    pred_idxs = range(start_idx + 1, start_idx + 1 + len(predictions))\n",
    "    plt.plot(pred_idxs, predictions, 'r--', linewidth=2, label='One-step Predictions')\n",
    "\n",
    "    # Plot extended predictions\n",
    "    ext_pred_idxs = range(start_idx + 1, start_idx + 1 + len(extended_predictions))\n",
    "    plt.plot(ext_pred_idxs, extended_predictions, 'm:', linewidth=2, label='Extended Predictions')\n",
    "\n",
    "    plt.title('LSTM Pattern Prediction')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Highlight the pattern repeats\n",
    "    for i in range(0, total_samples, pattern_length):\n",
    "        plt.axvline(x=i, color='g', alpha=0.2, linestyle=':')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Pattern prediction demonstration complete!\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Observations:\")\n",
    "    print(\"1. The LSTM learns to predict the repeating pattern\")\n",
    "    print(\"2. The model captures both the pattern and the slight noise in the data\")\n",
    "    print(\"3. For extended predictions, the LSTM maintains the pattern structure\")\n",
    "    print(\"   but may drift over time without correction\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pattern_example()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
